{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from nltk import classify\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "from statistics import mode\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive dataset dimension:  5500\n",
      "negative dataset dimension:  5500\n"
     ]
    }
   ],
   "source": [
    "# Positive tweets from dataset\n",
    "positive = pd.read_csv('..\\\\Datasets\\\\Training Datasets\\\\positive.csv', usecols=[\n",
    "    'tweet_text', 'sentiment'], engine='python')\n",
    "\n",
    "# Negative tweets from dataset\n",
    "negative = pd.read_csv('..\\\\Datasets\\\\Training Datasets\\\\negative.csv', usecols=[\n",
    "    'tweet_text', 'sentiment'], engine='python')\n",
    "\n",
    "print(\"positive dataset dimension: \", len(positive))\n",
    "print(\"negative dataset dimension: \", len(negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "* Tokenization\n",
    "* Stemmatization\n",
    "* Removal of italian stopwords\n",
    "* Removal of punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing positive, negative and text\n",
    "positive_tokens = positive['tweet_text'].apply(word_tokenize)\n",
    "negative_tokens = negative['tweet_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords and punctuation removal and Stemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Italian stopwords\n",
    "stop_words = stopwords.words('italian')\n",
    "\n",
    "# Italian Stemmer\n",
    "stemmer = SnowballStemmer('italian')\n",
    "\n",
    "\n",
    "# Additional stopwords found online\n",
    "def additional_stop_words():\n",
    "    with open('..\\\\Training\\\\stopwords.txt', 'r') as f:\n",
    "        additional_stopwords = f.readlines()\n",
    "    additional_stopwords = [x.strip() for x in additional_stopwords]\n",
    "    return additional_stopwords\n",
    "\n",
    "\n",
    "# Function to remove noise from tokens, removing also stopwords\n",
    "def remove_noise(tweet_tokens, stop_words=(), additional_stop_words=()):\n",
    "    cleaned_tokens = []\n",
    "    for token in tweet_tokens:\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n",
    "        token = stemmer.stem(token)\n",
    "        if len(token) > 3 and token not in string.punctuation and token.lower() not in stop_words and token.lower() not in additional_stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of positive and negative cleaned tokens\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "# Cleaning positive tokens and adding to list\n",
    "for tokens in positive_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "# Cleaning negative tokens and adding to list\n",
    "for tokens in negative_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a generator function that takes a list of tweets as an argument and\n",
    "# provides a list of words in all of the tweet tokens joined.\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive words\n",
    "every_positive_word = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "# Negative words\n",
    "every_negative_word = get_all_words(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('grand', 1088), ('graz', 745), ('sempr', 727), ('buon', 492), ('augur', 385), ('bell', 353), ('campion', 320), ('forz', 302), ('tutt', 302), ('brav', 277)]\n",
      "[('cazz', 587), ('merd', 507), ('vergogn', 479), ('part', 398), ('gioc', 386), ('fatt', 384), ('stat', 361), ('sempr', 354), ('tifos', 334), ('inter', 326)]\n"
     ]
    }
   ],
   "source": [
    "# What are the most positive words and how frequent are they?\n",
    "freq_dist_positive = FreqDist(every_positive_word)\n",
    "print(freq_dist_positive.most_common(10))\n",
    "\n",
    "freq_dist_negative = FreqDist(every_negative_word)\n",
    "print(freq_dist_negative.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a list of cleaned tokens to dictionaries\n",
    "# token as the key and True as values\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for training\n",
    "positive_tokens_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "# Attach label Positive or Negative to each tweet\n",
    "positive_dataset = [(tweet_dict, 'Positive')\n",
    "                    for tweet_dict in positive_tokens_model]\n",
    "negative_dataset = [(tweet_dict, 'Negative')\n",
    "                    for tweet_dict in negative_tokens_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset by joining positive and negative\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "# Shuffle the dataset to avoid bias\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Separating training to test data 70/30\n",
    "train_data = dataset[:9000]\n",
    "test_data = dataset[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with different algorithms\n",
    "* Naive Bayes\n",
    "* Logistic Regression\n",
    "* Bernoulli Naive Bayes\n",
    "* Multinomial Naibe Bayes\n",
    "* Stochastic Gradient Descent\n",
    "* Support Vector Classification\n",
    "* NuSVC\n",
    "* LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 84.0 %\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classifier\n",
    "NaiveBayes = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(NaiveBayes, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\NaiveBayes.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(NaiveBayes, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 88.0 %\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression classifier\n",
    "LRClassifier = SklearnClassifier(LogisticRegression())\n",
    "LRClassifier.train(train_data)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(LRClassifier, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\LRClassifier.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(LRClassifier, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 85.0 %\n"
     ]
    }
   ],
   "source": [
    "# BernoulliNB classifier\n",
    "BernoulliNB = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB.train(train_data)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(BernoulliNB, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\BernoulliNB.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(BernoulliNB, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 86.5 %\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB classifier\n",
    "MultinomialNB = SklearnClassifier(MultinomialNB())\n",
    "MultinomialNB.train(train_data)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(MultinomialNB, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\MultinomialNB.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(MultinomialNB, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 87.25 %\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier classifier\n",
    "SGDClassifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier.train(train_data)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(SGDClassifier, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\SGDClassifier.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(SGDClassifier, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 88.1 %\n"
     ]
    }
   ],
   "source": [
    "# SVC classifier\n",
    "SVC = SklearnClassifier(SVC())\n",
    "SVC.train(train_data)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(SVC, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\SVC.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(SVC, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 87.94999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# NuSVC\n",
    "NuSVC = SklearnClassifier(NuSVC())\n",
    "NuSVC.train(train_data)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(NuSVC, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\NuSVC.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(NuSVC, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 86.0 %\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC\n",
    "LinearSVC = SklearnClassifier(LinearSVC())\n",
    "LinearSVC.train(train_data)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(LinearSVC, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\LinearSVC.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(LinearSVC, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model\n",
    "This model combines the predictions from each model and uses the majority vote as the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininig the ensemble model class \n",
    "\n",
    "class EnsembleClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    # returns the classification based on majority of votes\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all classifiers from the pickled files\n",
    "\n",
    "# function to load models given filepath\n",
    "def load_model(file_path): \n",
    "    classifier_f = open(file_path, \"rb\")\n",
    "    classifier = pickle.load(classifier_f)\n",
    "    classifier_f.close()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an odd number of models to avoid the chance of a tie\n",
    "\n",
    "# Original Naive Bayes Classifier\n",
    "NB_clf = load_model('..\\\\Models\\\\NaiveBayes.pkl')\n",
    "\n",
    "# BernoulliNB\n",
    "BernoulliNB_clf = load_model('..\\\\Models\\\\BernoulliNB.pkl')\n",
    "\n",
    "# Multinomial Naive Bayes Classifier \n",
    "MNB_Clf = load_model('..\\\\Models\\\\MultinomialNB.pkl')\n",
    "\n",
    "# SVC Classifier \n",
    "SVC_Clf = load_model('..\\\\Models\\\\SVC.pkl')\n",
    "\n",
    "# Logistic Regression Classifier \n",
    "LogReg_Clf = load_model('..\\\\Models\\\\LRClassifier.pkl')\n",
    "\n",
    "# Stochastic Gradient Descent Classifier\n",
    "SGD_Clf = load_model('..\\\\Models\\\\MultinomialNB.pkl')\n",
    "\n",
    "# NuSVC\n",
    "NuSVC_clf = load_model('..\\\\Models\\\\NuSVC.pkl')\n",
    "\n",
    "# LienarSVC\n",
    "LinearSVC_clf = load_model('..\\\\Models\\\\LinearSVC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the ensemble classifier \n",
    "Ensemble = EnsembleClassifier(NB_clf, MNB_Clf, SVC_Clf, LogReg_Clf, SGD_Clf, NuSVC_clf, LinearSVC_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 88.6 %\n"
     ]
    }
   ],
   "source": [
    "# Testing new classifier\n",
    "\n",
    "print(\"Classifier accuracy percent:\",\n",
    "      (classify.accuracy(Ensemble, test_data))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file for later usage\n",
    "filename = '..\\\\Models\\\\Ensemble.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(Ensemble, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prediciton has improved by 0.5% using the Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 32-bit",
   "language": "python",
   "name": "python38232bit5a0ac0eee04341c5b31cb6e782f47838"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
