{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from nltk import classify\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model loading\n",
    "I will be using the Logistic Regression Classifier since it's the most accurate (87.9%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model with pickle\n",
    "LRClassifier = \"..\\\\Models\\\\LRClassifier.pkl\"\n",
    "with open(LRClassifier, 'rb') as file:\n",
    "    LRClassifier = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tweets from February\n",
    "tweets = pd.read_csv('..\\\\Datasets\\\\JSON\\\\February\\\\dataCleaned.csv', usecols=[\n",
    "    'data', 'text', 'date'], engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "* Tokenization\n",
    "* Stemmatization\n",
    "* Removal of italian stopwords\n",
    "* Removal of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Italian stopwords\n",
    "stop_words = stopwords.words('italian')\n",
    "\n",
    "# Italian Stemmer\n",
    "stemmer = SnowballStemmer('italian')\n",
    "\n",
    "\n",
    "# Additional stopwords found online\n",
    "def additional_stop_words():\n",
    "    with open('Training\\\\stopwords.txt', 'r') as f:\n",
    "        additional_stopwords = f.readlines()\n",
    "    additional_stopwords = [x.strip() for x in additional_stopwords]\n",
    "    return additional_stopwords\n",
    "\n",
    "\n",
    "# Function to remove noise from tokens, removing also stopwords\n",
    "def remove_noise(tweet_tokens, stop_words=(), additional_stop_words=()):\n",
    "    cleaned_tokens = []\n",
    "    for token in tweet_tokens:\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n",
    "        token = stemmer.stem(token)\n",
    "        if len(token) > 1 and token not in string.punctuation and token.lower() not in stop_words and token.lower() not in additional_stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding tweets and tokenized from february to list\n",
    "february = tweets.data.values.tolist()\n",
    "tokenized = tweets.text.values.tolist()\n",
    "date = tweets.date.values.tolist()\n",
    "# List of classified tweets from april\n",
    "classified = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each tweet, remove noise and tokenize, calculate the accuracy of a given prediction\n",
    "# and add to classified list\n",
    "for tweet in february:\n",
    "    custom_tokens = remove_noise(word_tokenize(tweet))\n",
    "    classified.append(tuple((tweet, LRClassifier.prob_classify(\n",
    "        dict([token, True] for token in custom_tokens)).prob('Positive'), LRClassifier.prob_classify(\n",
    "        dict([token, True] for token in custom_tokens)).prob('Negative'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe from classified tweets\n",
    "df = pd.DataFrame(classified, columns=['tweet', 'positive', 'negative'])\n",
    "\n",
    "# Obtain polarity by subtracting positives with negatives values\n",
    "df['polarity'] = df['positive'] - df['negative']\n",
    "\n",
    "# Adding tokenized column\n",
    "df['tokenized'] = tokenized\n",
    "\n",
    "# Adding date column\n",
    "df['date'] = date\n",
    "\n",
    "# Reordering columns\n",
    "df = df[['date', 'tweet', 'tokenized', 'positive', 'negative', 'polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date  \\\n",
      "0  2020-02-01 23:59:39+00:00   \n",
      "1  2020-02-01 23:59:39+00:00   \n",
      "2  2020-02-01 23:59:39+00:00   \n",
      "3  2020-02-01 23:59:38+00:00   \n",
      "4  2020-02-01 23:59:38+00:00   \n",
      "5  2020-02-01 23:59:38+00:00   \n",
      "6  2020-02-01 23:59:38+00:00   \n",
      "7  2020-02-01 23:59:38+00:00   \n",
      "8  2020-02-01 23:59:35+00:00   \n",
      "9  2020-02-01 23:59:18+00:00   \n",
      "\n",
      "                                               tweet  \\\n",
      "0  Coronavirus i morti salgono a 304 In un giorno...   \n",
      "1  Coronavirus sale a 259 il numero dei morti 18 ...   \n",
      "2  Coronavirus venerd record con 46 morti e 2102 ...   \n",
      "3  Coronavirus tutto quello che sappiamo e ci che...   \n",
      "4  Si vabb ci sono pi morti per femminicidio o pe...   \n",
      "5  Coronavirus gi 12mila casi nel mondo i dieci P...   \n",
      "6  Coronavirus la mascherina serve ma se indossat...   \n",
      "7  Coronavirus dopo i due casi accertati 32 perso...   \n",
      "8  Se avete un parente un amico o un conoscente c...   \n",
      "9  Quando tutti sono preoccupati per il corona vi...   \n",
      "\n",
      "                                           tokenized  positive  negative  \\\n",
      "0  coronavirus morti salgono 304 giorno 45 decess...  0.332409  0.667591   \n",
      "1   coronavirus sale 259 numero morti 18 casi europa  0.581793  0.418207   \n",
      "2  coronavirus venerd record 46 morti 2102 nuovi ...  0.274135  0.725865   \n",
      "3           coronavirus sappiamo resta ancora capire  0.789562  0.210438   \n",
      "4  vabb pi morti femminicidio incidenti stradali ...  0.295607  0.704393   \n",
      "5  coronavirus gi 12mila casi mondo dieci paesi p...  0.479477  0.520523   \n",
      "6        coronavirus mascherina serve indossata male  0.499129  0.500871   \n",
      "7  coronavirus dopo due casi accertati 32 persone...  0.525625  0.474375   \n",
      "8  parente amico conoscente andando pi mangiare r...  0.197923  0.802077   \n",
      "9  quando preoccupati corona virus no perch tanto...  0.432237  0.567763   \n",
      "\n",
      "   polarity  \n",
      "0 -0.335181  \n",
      "1  0.163586  \n",
      "2 -0.451731  \n",
      "3  0.579123  \n",
      "4 -0.408787  \n",
      "5 -0.041046  \n",
      "6 -0.001742  \n",
      "7  0.051251  \n",
      "8 -0.604153  \n",
      "9 -0.135527  \n"
     ]
    }
   ],
   "source": [
    "# Quick glance at the dataframe\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataframe to csv\n",
    "df.to_csv('..\\\\Datasets\\\\CSV\\\\february_analyzed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
